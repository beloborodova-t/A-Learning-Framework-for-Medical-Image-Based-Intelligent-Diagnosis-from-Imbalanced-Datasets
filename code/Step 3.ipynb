{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsPvacJxjbBI"
      },
      "outputs": [],
      "source": [
        "pretrained_model_name = 'ResNet152_ImageNet_Caffe.model'\n",
        "pretrained_model_path = 'D:/...models_path/'\n",
        "pretrained_node_name = 'pool5' \n",
        "\n",
        "dirs = [\"0\", \"1\", \"2\", \"3\"] \n",
        "data_path = 'D:/...data_path/'\n",
        "\n",
        "image_height = 224 \n",
        "image_width  = 224 \n",
        "num_channels = 3 \n",
        "random_seed = 5\n",
        "train_ratio = 0.8 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qS3eeYimjbBO"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cntk as C\n",
        "from PIL import Image\n",
        "import pickle\n",
        "import time\n",
        "from cntk import load_model, combine\n",
        "import cntk.io.transforms as xforms\n",
        "from cntk.logging import graph\n",
        "from cntk.logging.graph import get_node_outputs\n",
        "\n",
        "picklefolder_path = os.path.join(data_path, 'pickle') \n",
        "if not os.path.exists(picklefolder_path):\n",
        "    os.mkdir(picklefolder_path)\n",
        "\n",
        "output_path = 'D:/...models_path/'\n",
        "if not os.path.exists(output_path):\n",
        "    os.mkdir(output_path)\n",
        "    \n",
        "regression_model_path = os.path.join(output_path, 'cntk_regression.dat')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eABzz2eSjbBR"
      },
      "outputs": [],
      "source": [
        "model_file  = os.path.join(pretrained_model_path, pretrained_model_name)\n",
        "loaded_model  = load_model(model_file) # pretrained ResNet-152 model\n",
        "node_in_graph = loaded_model.find_by_name(pretrained_node_name)\n",
        "output_nodes  = combine([node_in_graph.owner])\n",
        "\n",
        "node_outputs = C.logging.get_node_outputs(loaded_model)\n",
        "for l in node_outputs: \n",
        "    if l.name == pretrained_node_name:\n",
        "        num_nodes = np.prod(np.array(l.shape))\n",
        "        \n",
        "print ('the pretrained model is %s' % pretrained_model_name)\n",
        "print ('the selected layer name is %s and the number of flatten nodes is %d' % (pretrained_node_name, num_nodes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k87e7-nYjbBV"
      },
      "outputs": [],
      "source": [
        "def extract_features(image_path):   \n",
        "    img = Image.open(image_path)       \n",
        "    resized = img.resize((image_width, image_height), Image.ANTIALIAS)  \n",
        "    \n",
        "    bgr_image = np.asarray(resized, dtype=np.float32)[..., [2, 1, 0]]    \n",
        "    hwc_format = np.ascontiguousarray(np.rollaxis(bgr_image, 2)) \n",
        "    \n",
        "    arguments = {loaded_model.arguments[0]: [hwc_format]}    \n",
        "    output = output_nodes.eval(arguments)  \n",
        "    return output\n",
        "\n",
        "def maybe_pickle(folder_path): \n",
        "    dataset = np.ndarray(shape=(len(next(os.walk(folder_path))[2]), num_nodes),\n",
        "                         dtype=np.float16) \n",
        "    num_image = 0        \n",
        "    for file in next(os.walk(folder_path))[2]:\n",
        "        image_path = os.path.join(folder_path, file)\n",
        "        dataset[num_image, :] = extract_features(image_path)[0].flatten()\n",
        "        num_image = num_image + 1\n",
        "    \n",
        "    pickle_filename = folder_path.split('\\\\')[-1] + '.pickle'\n",
        "    pickle_filepath = os.path.join(picklefolder_path, pickle_filename)\n",
        "    if os.path.isfile(pickle_filepath):\n",
        "        os.remove(pickle_filepath)\n",
        "    with open(pickle_filepath, 'wb') as f:\n",
        "        pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL) \n",
        "    \n",
        "    return pickle_filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htDiy2rnjbBX"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "\n",
        "pickle_names = []\n",
        "    \n",
        "for f in dirs:\n",
        "    folder_path = os.path.join(data_path, f)\n",
        "    pickle_names.append(os.path.join(picklefolder_path, maybe_pickle(folder_path)))\n",
        "\n",
        "print(\"It takes %s seconds to extract features from skin patch images and dump to pickle files.\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qe9-DlO-jbBZ"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "def merge_datasets(pickle_files, train_ratio):\n",
        "    num_classes = len(pickle_files)\n",
        "    num_datasets = [0]*num_classes\n",
        "    for i in range(num_classes):\n",
        "        with open(pickle_files[i], 'rb') as f:\n",
        "            load_data = pickle.load(f)\n",
        "            num_datasets[i] = load_data.shape[0]\n",
        "            \n",
        "    total_datasets = np.sum(num_datasets)\n",
        "    \n",
        "    num_train = [int(round(float(x)*train_ratio)) for x in num_datasets]\n",
        "    num_valid = np.array(num_datasets) - np.array(num_train)\n",
        "   \n",
        "    total_train = np.sum(num_train)\n",
        "    train_dataset = np.ndarray((total_train, num_nodes), dtype=np.float32)\n",
        "    train_labels = np.ndarray(total_train, dtype=np.int32)  \n",
        "    \n",
        "    total_valid = np.sum(num_valid)\n",
        "    valid_dataset = np.ndarray((total_valid, num_nodes), dtype=np.float32)\n",
        "    valid_labels = np.ndarray(total_valid, dtype=np.int32)  \n",
        "    \n",
        "    start_trn, start_val = 0, 0\n",
        "    np.random.seed(seed=random_seed)\n",
        "    for label, pickle_file in enumerate(pickle_files):  \n",
        "        print (label+1)\n",
        "        print (pickle_file)\n",
        "        try:\n",
        "            with open(pickle_file, 'rb') as f:\n",
        "                data_set = pickle.load(f)\n",
        "                np.random.shuffle(data_set) \n",
        "                \n",
        "                train_data = data_set[0:num_train[label], :]\n",
        "                train_dataset[start_trn:(start_trn+num_train[label]), :] = train_data\n",
        "                train_labels[start_trn:(start_trn+num_train[label])] = label+1\n",
        "                start_trn += num_train[label]\n",
        "                \n",
        "                valid_data = data_set[num_train[label]:num_datasets[label], :]\n",
        "                valid_dataset[start_val:(start_val+num_valid[label]), :] = valid_data\n",
        "                valid_labels[start_val:(start_val+num_valid[label])] = label+1\n",
        "                start_val += num_valid[label]\n",
        "\n",
        "        except Exception as e:\n",
        "            print('Unable to process data from', pickle_file, ':', e)\n",
        "            raise   \n",
        "    X_train_res, y_train_res = SMOTE().fit_resample(train_dataset, train_labels.ravel())   \n",
        "    X_valid_res, y_valid_res = SMOTE().fit_resample(valid_dataset, valid_labels.ravel())         \n",
        "    return X_train_res, y_train_res, X_valid_res, y_valid_res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCIE-ZcOjbBb"
      },
      "outputs": [],
      "source": [
        "pickle_names=[\"D:/...data_path/pickle/0.pickle\",\"D:/...data_path/pickle/1.pickle\",\"D:/...data_path/pickle/2.pickle\",\"D:/...data_path/pickle/3.pickle\"]\n",
        "train_ratio = 0.8\n",
        "train_dataset, train_labels, valid_dataset, valid_labels = merge_datasets(pickle_names, train_ratio)\n",
        "print('Training:', train_dataset.shape, train_labels.shape)\n",
        "print('Validation:', valid_dataset.shape, valid_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcn3Gjb3jbBd"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "clf_regr = MLPRegressor(hidden_layer_sizes=(1024, 512, 256),activation='tanh', solver='lbfgs',max_iter=400)\n",
        "clf_regr.fit(train_dataset, train_labels) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqaDWxizjbBf"
      },
      "outputs": [],
      "source": [
        "pred_labels_regr = clf_regr.predict(valid_dataset)\n",
        "print(pred_labels_regr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHADo51ZjbBg"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "rmse_regr = sqrt(mean_squared_error(pred_labels_regr, valid_labels))\n",
        "print ('the RMSE of regression NN is %f' % rmse_regr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xjh5OxNIjbBi"
      },
      "outputs": [],
      "source": [
        "errors = mean_squared_error(valid_labels, pred_labels_regr)\n",
        "print ('the MSE of regression NN is %f' % errors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZPG12rujbBj"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "errors = mean_absolute_error(valid_labels, pred_labels_regr)\n",
        "print ('the MAE of regression NN is %f' % errors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wl0nkd89jbBk"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import explained_variance_score\n",
        "print(explained_variance_score(valid_labels, pred_labels_regr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eX3BHphjbBl"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import max_error\n",
        "print(max_error(valid_labels, pred_labels_regr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V42F5QJKjbBm"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_log_error\n",
        "print(mean_squared_log_error(valid_labels, pred_labels_regr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLpnHNvojbBm"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "print(mean_absolute_percentage_error(valid_labels, pred_labels_regr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4ivmmeIjbBn"
      },
      "outputs": [],
      "source": [
        " from sklearn.metrics import median_absolute_error\n",
        " print(median_absolute_error(valid_labels, pred_labels_regr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Svcr-ImwjbBn"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import r2_score\n",
        "print(r2_score(valid_labels, pred_labels_regr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1ylV4gpjbBo"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_tweedie_deviance\n",
        "print(mean_tweedie_deviance(valid_labels, pred_labels_regr, power=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMQGBRRhjbBp"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_tweedie_deviance\n",
        "print(mean_tweedie_deviance(valid_labels, pred_labels_regr, power=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NP5zzYPkjbBp"
      },
      "outputs": [],
      "source": [
        "regr_model = pickle.dumps(clf_regr)\n",
        "regression_store= pd.DataFrame({\"model\":[regr_model]})\n",
        "regression_store.to_pickle(regression_model_path)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.6.8 64-bit",
      "name": "python368jvsc74a57bd047a396712524a9f40c0989d686d7e08acdccf13c179afe16abd65e75ca53c1e0"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "metadata": {
      "interpreter": {
        "hash": "47a396712524a9f40c0989d686d7e08acdccf13c179afe16abd65e75ca53c1e0"
      }
    },
    "colab": {
      "name": "Step4,5.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
